
import os
from llama_index.core import SimpleDirectoryReader, StorageContext, VectorStoreIndex, load_index_from_storage
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.text_splitter import TokenTextSplitter

# Ruta a la carpeta de almacenamiento del Ã­ndice
persist_dir = "storage"

# Verifica si ya existe un Ã­ndice guardado
if os.path.exists(os.path.join(persist_dir, "docstore.json")):
    print("ðŸ“¦ Cargando Ã­ndice existente desde disco...")
    storage_context = StorageContext.from_defaults(persist_dir=persist_dir)
    index = load_index_from_storage(storage_context)
else:
    print("ðŸ†• No se encontrÃ³ Ã­ndice existente. Generando nuevo Ã­ndice...")
    documents = SimpleDirectoryReader("docs").load_data()
    
    # Usamos un TextSplitter con configuraciÃ³n avanzada (puedes ajustar valores)
    splitter = TokenTextSplitter(separator="\n", chunk_size=512, chunk_overlap=100)
    
    # Embeddings avanzados (OpenAI por defecto)
    embed_model = OpenAIEmbedding(model="text-embedding-3-small")  # Ajustable

    index = VectorStoreIndex.from_documents(
        documents,
        embed_model=embed_model,
        transformations=[splitter]
    )

    # Guardamos el Ã­ndice
    index.storage_context.persist(persist_dir=persist_dir)

print("âœ… Proceso de indexaciÃ³n completado.")
